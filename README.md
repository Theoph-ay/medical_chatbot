<div align="center">

# ü©∫ MedAssist AI ‚Äî Medical Chatbot

**An AI-powered medical assistant built on Hutchinson's Clinical Methods**

Retrieval-Augmented Generation (RAG) chatbot that answers evidence-based medical questions using vector-searched context from Hutchinson's clinical method textbook.

[![Python](https://img.shields.io/badge/Python-3.14+-3776AB?logo=python&logoColor=white)](https://python.org)
[![Flask](https://img.shields.io/badge/Flask-3.1+-000000?logo=flask&logoColor=white)](https://flask.palletsprojects.com)
[![LangChain](https://img.shields.io/badge/LangChain-1.2+-1C3C3C?logo=langchain&logoColor=white)](https://langchain.com)
[![Pinecone](https://img.shields.io/badge/Pinecone-Vector_DB-00C5CD)](https://pinecone.io)

---

![MedAssist AI Dark UI](https://img.shields.io/badge/UI-Dark_Theme-1F2937?style=for-the-badge)

</div>

## ‚ú® Features

- ü§ñ **RAG-powered answers** ‚Äî retrieves the most relevant chunks from medical textbooks before generating a response
- üìö **Hutchinson's Clinical Methods** ‚Äî built on one of the most trusted clinical reference texts
- üß† **Groq LLM inference** ‚Äî fast responses via the Groq API
- üå≤ **Pinecone vector store** ‚Äî semantic search across chunked medical documents
- üé® **Beautiful dark-themed UI** ‚Äî modern, responsive chat interface with markdown & table rendering
- ‚ö° **Real-time chat** ‚Äî typing indicators, animated transitions, and quick-prompt suggestions

## üèóÔ∏è Architecture

```
User Question
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Flask    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  LangChain    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Pinecone   ‚îÇ
‚îÇ  Frontend ‚îÇ     ‚îÇ  RAG Chain    ‚îÇ     ‚îÇ  Vector DB   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ   Groq LLM   ‚îÇ
                  ‚îÇ  (GPT-oss)   ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
                   AI Response
```

## üìÅ Project Structure

```
medical_chatbot/
‚îú‚îÄ‚îÄ app.py                 # Flask application & routes
‚îú‚îÄ‚îÄ store_index.py         # Script to ingest PDFs into Pinecone
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ helper.py          # PDF loading, text splitting, embeddings
‚îÇ   ‚îî‚îÄ‚îÄ prompt.py          # System prompt template
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ chat.html          # Chat UI (HTML + JavaScript)
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ style.css          # Dark-themed stylesheet
‚îú‚îÄ‚îÄ data/                  # Place your PDF textbooks here
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ setup.py
‚îî‚îÄ‚îÄ .env                   # API keys (not committed)
```

## üöÄ Getting Started

### Prerequisites

- **Python 3.14+**
- A [**Pinecone**](https://pinecone.io) account (free tier works)
- A [**Groq**](https://console.groq.com) API key

### 1. Clone the Repository

```bash
git clone https://github.com/Theoph-ay/medical_chatbot.git
cd medical_chatbot
```

### 2. Create a Virtual Environment

```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# macOS / Linux
python3 -m venv .venv
source .venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Set Up Environment Variables

Create a `.env` file in the project root:

```env
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=your_index_name_here
GROQ_API_KEY=your_groq_api_key_here
```

> **Note:** You need to create a Pinecone index first via the [Pinecone console](https://app.pinecone.io). Use **384 dimensions** (matching the `all-MiniLM-L6-v2` embedding model) with **cosine** metric.

### 5. Ingest Your Medical PDFs

Place your PDF files (e.g., Hutchinson's Clinical Methods) into the `data/` folder, then run:

```bash
python store_index.py
```

This will:
- Load all PDFs from the `data/` directory
- Split them into 500-character chunks with 20-character overlap
- Generate embeddings using `sentence-transformers/all-MiniLM-L6-v2`
- Upload the vectors to your Pinecone index

### 6. Run the Application

```bash
python app.py
```

Open your browser and navigate to:

```
http://127.0.0.1:5000
```

## üõ†Ô∏è Tech Stack

| Technology | Purpose |
|---|---|
| **Python** | Core language |
| **Flask** | Web framework & API server |
| **LangChain** | RAG orchestration (retrieval chain + stuff documents chain) |
| **Pinecone** | Vector database for semantic search |
| **Groq** | LLM inference (fast cloud-based generation) |
| **Sentence Transformers** | Embedding model (`all-MiniLM-L6-v2`) |
| **PyPDF** | PDF document loading |
| **HTML/CSS/JS** | Frontend chat interface |

## üìù How It Works

1. **PDF Ingestion** ‚Äî `store_index.py` reads medical PDFs, splits them into chunks, embeds them, and stores vectors in Pinecone.
2. **User Query** ‚Äî The user types a medical question in the chat UI.
3. **Retrieval** ‚Äî LangChain's retriever fetches the top 6 most relevant chunks from Pinecone.
4. **Generation** ‚Äî The retrieved context + question are passed to the Groq LLM, which generates a clinical, evidence-based answer.
5. **Display** ‚Äî The response is rendered in the chat with markdown formatting, including tables, lists, and code blocks.

## ‚ö†Ô∏è Disclaimer

> MedAssist AI is designed for **educational and informational purposes only**. It is not a substitute for professional medical advice, diagnosis, or treatment. Always consult a qualified healthcare provider for medical decisions.

## üìÑ License

This project is open source.

---

<div align="center">

**Built with ‚ù§Ô∏è using Python, Flask, LangChain & Pinecone**

</div>
