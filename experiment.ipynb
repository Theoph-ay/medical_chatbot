{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b00494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Loba\\medical_chatbot\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "c:\\Users\\Loba\\medical_chatbot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import pinecone\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd55ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pinecone details\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_index_name = os.getenv(\"PINECONE_INDEX_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9425a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = PyPDFDirectoryLoader(data)\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05839b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ae4d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9506ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    test_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "    test_chunks = test_splitter.split_documents(extracted_data)\n",
    "    return test_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1cb31c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3769"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9ec42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    emebeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return emebeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268b1d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loba\\AppData\\Local\\Temp\\ipykernel_10892\\2663670125.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emebeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 181.05it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "102a6b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d8bb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PINECONE_API_KEY\"] = pinecone_api_key\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents = text_chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=pinecone_index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a0eb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert medical assistant.\n",
    "Use the following pieces of context (retrieved from medical slides) to answer the question.\n",
    "\n",
    "If the answer is not in the context, say \"I cannot find the answer in the provided slides.\"\n",
    "Keep your answer concise and clinical.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58054895",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "953eaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    api_key = os.getenv(groq_api_key),\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19a9ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e4bd4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e08d8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eda45f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I cannot find the answer in the provided slides.\n",
      "Response: I cannot find the answer in the provided slides.\n",
      "Response: I cannot find the answer in the provided slides.\n",
      "Response: I cannot find the answer in the provided slides.\n",
      "Response: Tuberculosis is a disease of the respiratory system.\n",
      "Response: Tuberculosis infection of the pleura elicits a delayed‑type hypersensitivity reaction that produces a lymphocyte‑rich exudative effusion. The fluid is typically an exudate (high protein) with many lymphocytes; tubercle bacilli are seldom seen directly. The immune response leads to granulomatous inflammation, which can be confirmed by pleural biopsy. When the organism involves the spine, it causes tuberculous discitis with destruction of the inter‑vertebral disc and adjacent vertebral bodies, producing deformities such as a gibbus.\n",
      "Response: When you are ready to finish the interview, follow these steps:\n",
      "\n",
      "1. **Give a clear cue** – tell the patient that the session is coming to an end so it isn’t abrupt.  \n",
      "2. **Invite any remaining concerns** – ask, “Is there anything else you’d like to discuss or that you feel is important that we haven’t covered?”  \n",
      "3. **Summarize key points** – briefly recap what was discussed and any agreed‑upon plans.  \n",
      "4. **Explain follow‑up** – let the patient know that a report will be sent to the referring physician and to his/her primary‑care doctor.  \n",
      "5. **Close with reassurance** – confirm that the patient understands the next steps and thank them for their time.\n",
      "Closing the medical chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"\\nInput Prompt: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Closing the medical chatbot. Goodbye!\")\n",
    "        break\n",
    "        \n",
    "    result = rag_chain.invoke({\"input\": user_input})\n",
    "    \n",
    "    # 4. Print the result\n",
    "    print(\"Response:\", result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
